{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['mosaicml/mpt-7b-8k-instruct', 'EleutherAI/gpt-neox-20b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EleutherAI/gpt-neox-20b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "877fc211b3b74967b64e9f4de2359a5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig\n",
    "\n",
    "model_name = model_names[1]\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)\n",
    "#config.attn_config['attn_impl'] = 'triton'  # change this to use triton-based FlashAttention\n",
    "config.init_device = 'cuda:0' # For fast initialization directly on GPU!\n",
    "\n",
    "print (model_name)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ") \n",
    "# ~4GB for MPT-7b\n",
    "# ~14GB for gpt-neox-20b\n",
    "\n",
    "#bnb_config = None \n",
    "# ~13 GB for MPT-7b\n",
    "# OOM for gpt-neox-20b\n",
    "\n",
    "# What happens if I pass bfloat16 and Bitsandbytes config to the model?\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  model_name,\n",
    "  config=config,\n",
    "  torch_dtype=torch.bfloat16, # Load model weights in bfloat16\n",
    "  trust_remote_code=True,\n",
    "  quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a 10 day iternary to Machu Pichu:\n",
      "\n",
      "1.  From Cusco, take a bus to Aguas Calientes.\n",
      "\n",
      "2.  From Aguas Calientes, take a bus to Machu Pichu.\n",
      "\n",
      "3.  From Machu Pichu, take a bus to Cusco.\n",
      "\n",
      "4.  From Cusco, take a bus to Aguas Calientes.\n",
      "\n",
      "5.  From Aguas Calientes, take a bus to Machu Pichu.\n",
      "\n",
      "6.  From Machu Pichu, take a bus to Cusco.\n",
      "\n",
      "7.  From Cusco, take a bus to Aguas Calientes.\n",
      "\n",
      "8.  From Aguas Calientes, take a bus to Machu Pichu.\n",
      "\n",
      "9.  From Machu Pichu, take a bus to Cusco.\n",
      "\n",
      "10.  From Cusco, take a bus to Aguas Calientes.\n",
      "\n",
      "The bus from Machu Pichu to Cusco is a very long and winding road.  The bus from Cusco to Aguas Calientes is a very long and winding road.  The bus from Aguas Calientes to Machu Pichu is a very long and winding road.  The bus from Machu Pichu to Cusco is a very long and winding road.  The bus from Cusco to Aguas Calientes is a very long and winding road.  The bus from Aguas Calientes to Machu Pichu is a very long and winding road.  The bus from Machu Pichu to Cusco is a very long and winding road.  The bus from Cusco to Aguas Calientes is a very long and winding road.  The bus from Aguas Calientes to Machu Pichu is a very long and winding road.  The bus from Machu Pichu to Cus\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "with torch.autocast('cuda', dtype=torch.bfloat16):\n",
    "    inputs = tokenizer('Here is a 10 day iternary to Machu Pichu:\\n', return_tensors=\"pt\").to('cuda')\n",
    "    outputs = model.generate(**inputs, max_new_tokens=400)\n",
    "    print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QLORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.modules of GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(50432, 6144)\n",
       "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-43): 44 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((6144,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((6144,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "          (query_key_value): Linear4bit(in_features=6144, out_features=18432, bias=True)\n",
       "          (dense): Linear4bit(in_features=6144, out_features=6144, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear4bit(in_features=6144, out_features=24576, bias=True)\n",
       "          (dense_4h_to_h): Linear4bit(in_features=24576, out_features=6144, bias=True)\n",
       "          (act): FastGELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((6144,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (embed_out): Linear(in_features=6144, out_features=50432, bias=False)\n",
       ")>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.modules"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PEFT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 8,650,752 || all params: 10,597,552,128 || trainable%: 0.08162971878329976\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8, \n",
    "    lora_alpha=32, \n",
    "    target_modules=[\"query_key_value\"],\n",
    "    lora_dropout=0.05, \n",
    "    bias=\"none\", \n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/siddhesh1793/.cache/huggingface/datasets/Abirate___json/Abirate--english_quotes-6e72855d06356857/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "593a1bb19e444fb599aa5102e5ed1d89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/siddhesh1793/.cache/huggingface/datasets/Abirate___json/Abirate--english_quotes-6e72855d06356857/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8d82074056c19c4a.arrow\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"Abirate/english_quotes\")\n",
    "data = data.map(lambda samples: tokenizer(samples[\"quote\"]), batched=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01b6f71946984975a66deac1c4f75b94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1369, 'learning_rate': 0.0001, 'epoch': 0.0}\n",
      "{'loss': 2.4352, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
      "{'loss': 2.4872, 'learning_rate': 0.000175, 'epoch': 0.0}\n",
      "{'loss': 3.1341, 'learning_rate': 0.00015000000000000001, 'epoch': 0.01}\n",
      "{'loss': 2.4746, 'learning_rate': 0.000125, 'epoch': 0.01}\n",
      "{'loss': 1.5631, 'learning_rate': 0.0001, 'epoch': 0.01}\n",
      "{'loss': 2.1726, 'learning_rate': 7.500000000000001e-05, 'epoch': 0.01}\n",
      "{'loss': 2.5679, 'learning_rate': 5e-05, 'epoch': 0.01}\n",
      "{'loss': 1.6739, 'learning_rate': 2.5e-05, 'epoch': 0.01}\n",
      "{'loss': 1.9144, 'learning_rate': 0.0, 'epoch': 0.02}\n",
      "{'train_runtime': 42.2753, 'train_samples_per_second': 0.946, 'train_steps_per_second': 0.237, 'train_loss': 2.255991780757904, 'epoch': 0.02}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10, training_loss=2.255991780757904, metrics={'train_runtime': 42.2753, 'train_samples_per_second': 0.946, 'train_steps_per_second': 0.237, 'train_loss': 2.255991780757904, 'epoch': 0.02})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "# needed for gpt-neo-x tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=data[\"train\"],\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=2,\n",
    "        max_steps=10,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=1,\n",
    "        output_dir=\"outputs\",\n",
    "        optim=\"paged_adamw_8bit\"\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
