{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['mosaicml/mpt-7b-8k-instruct', 'EleutherAI/gpt-neox-20b', 'openlm-research/open_llama_13b', 'meta-llama/Llama-2-13b-hf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EleutherAI/gpt-neox-20b\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "708dd6471a454dc1b1dcb991a1a62130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, BitsAndBytesConfig\n",
    "\n",
    "model_name = model_names[1]\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)\n",
    "#config.attn_config['attn_impl'] = 'triton'  # change this to use triton-based FlashAttention\n",
    "config.init_device = 'cuda:0' # For fast initialization directly on GPU!\n",
    "\n",
    "print (model_name)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ") \n",
    "# ~4GB for MPT-7b\n",
    "# ~14GB for gpt-neox-20b\n",
    "\n",
    "#bnb_config = None \n",
    "# ~13 GB for MPT-7b\n",
    "# OOM for gpt-neox-20b\n",
    "\n",
    "# What happens if I pass bfloat16 and Bitsandbytes config to the model?\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  model_name,\n",
    "  config=config,\n",
    "  torch_dtype=torch.bfloat16, # Load model weights in bfloat16\n",
    "  trust_remote_code=True,\n",
    "  quantization_config=bnb_config\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a 10 day iternary to Machu Pichu:\n",
      "\n",
      "Day 1:\n",
      "\n",
      "Arrive in Cusco.\n",
      "\n",
      "Day 2:\n",
      "\n",
      "Arrive in Cusco.\n",
      "\n",
      "Day 3:\n",
      "\n",
      "Cusco to Machu Pichu.\n",
      "\n",
      "Day 4:\n",
      "\n",
      "Machu Pichu.\n",
      "\n",
      "Day 5:\n",
      "\n",
      "Machu Pichu.\n",
      "\n",
      "Day 6:\n",
      "\n",
      "Machu Pichu.\n",
      "\n",
      "Day 7:\n",
      "\n",
      "Machu Pichu.\n",
      "\n",
      "Day 8:\n",
      "\n",
      "Machu Pichu.\n",
      "\n",
      "Day 9:\n",
      "\n",
      "Machu Pichu.\n",
      "\n",
      "Day 10:\n",
      "\n",
      "Machu Pichu.\n",
      "\n",
      "Day 11:\n",
      "\n",
      "Machu Pichu.\n",
      "\n",
      "Day 12:\n",
      "\n",
      "Machu Pichu.\n",
      "\n",
      "Day 13:\n",
      "\n",
      "Machu Pichu.\n",
      "\n",
      "Day 14:\n",
      "\n",
      "Machu Pichu.\n",
      "\n",
      "Day 15:\n",
      "\n",
      "Machu Pichu.\n",
      "\n",
      "Day 16:\n",
      "\n",
      "Machu Pichu.\n",
      "\n",
      "Day 17:\n",
      "\n",
      "Machu Pichu.\n",
      "\n",
      "Day 18:\n",
      "\n",
      "Machu Pichu.\n",
      "\n",
      "Day 19:\n",
      "\n",
      "Machu Pichu.\n",
      "\n",
      "Day 20:\n",
      "\n",
      "Machu Pichu.\n",
      "\n",
      "Day 21:\n",
      "\n",
      "Machu Pichu.\n",
      "\n",
      "Day 22:\n",
      "\n",
      "Machu Pichu.\n",
      "\n",
      "Day 23:\n",
      "\n",
      "Machu Pichu.\n",
      "\n",
      "Day 24:\n",
      "\n",
      "Machu Pichu.\n",
      "\n",
      "Day 25:\n",
      "\n",
      "Machu Pichu.\n",
      "\n",
      "Day 26:\n",
      "\n",
      "Machu Pichu.\n",
      "\n",
      "Day 27:\n",
      "\n",
      "Machu Pichu.\n",
      "\n",
      "Day 28:\n",
      "\n",
      "Machu Pichu.\n",
      "\n",
      "Day 29:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.autocast('cuda', dtype=torch.bfloat16):\n",
    "    inputs = tokenizer('Here is a 10 day iternary to Machu Pichu:\\n', return_tensors=\"pt\").to('cuda')\n",
    "    outputs = model.generate(**inputs, max_new_tokens=400)\n",
    "    print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QLORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.modules of GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(50432, 6144)\n",
       "    (emb_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-43): 44 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((6144,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((6144,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (post_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): GPTNeoXRotaryEmbedding()\n",
       "          (query_key_value): Linear4bit(in_features=6144, out_features=18432, bias=True)\n",
       "          (dense): Linear4bit(in_features=6144, out_features=6144, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear4bit(in_features=6144, out_features=24576, bias=True)\n",
       "          (dense_4h_to_h): Linear4bit(in_features=24576, out_features=6144, bias=True)\n",
       "          (act): FastGELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((6144,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (embed_out): Linear(in_features=6144, out_features=50432, bias=False)\n",
       ")>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.modules"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PEFT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 8,650,752 || all params: 10,597,552,128 || trainable%: 0.08162971878329976\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8, \n",
    "    lora_alpha=32, \n",
    "    lora_dropout=0.05, \n",
    "    bias=\"none\", \n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/siddhesh1793/.cache/huggingface/datasets/Abirate___json/Abirate--english_quotes-6e72855d06356857/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bdee7dee1f34f039785405c3c373d16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/siddhesh1793/.cache/huggingface/datasets/Abirate___json/Abirate--english_quotes-6e72855d06356857/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-8d82074056c19c4a.arrow\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"Abirate/english_quotes\")\n",
    "data = data.map(lambda samples: tokenizer(samples[\"quote\"]), batched=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e2ad1346a69477e955ce2e0071bda27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/siddhesh1793/code/llm_efficiency_challenge/llm_efficiency_challenge/.venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:428: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1369, 'learning_rate': 0.0001, 'epoch': 0.0}\n",
      "{'loss': 2.4352, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siddhesh1793/code/llm_efficiency_challenge/llm_efficiency_challenge/.venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:428: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4952, 'learning_rate': 0.00019583333333333334, 'epoch': 0.0}\n",
      "{'loss': 3.1632, 'learning_rate': 0.00019166666666666667, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siddhesh1793/code/llm_efficiency_challenge/llm_efficiency_challenge/.venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:428: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4205, 'learning_rate': 0.0001875, 'epoch': 0.01}\n",
      "{'loss': 1.5876, 'learning_rate': 0.00018333333333333334, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siddhesh1793/code/llm_efficiency_challenge/llm_efficiency_challenge/.venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:428: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1386, 'learning_rate': 0.0001791666666666667, 'epoch': 0.01}\n",
      "{'loss': 2.6409, 'learning_rate': 0.000175, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siddhesh1793/code/llm_efficiency_challenge/llm_efficiency_challenge/.venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:428: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6992, 'learning_rate': 0.00017083333333333333, 'epoch': 0.01}\n",
      "{'loss': 1.9618, 'learning_rate': 0.0001666666666666667, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siddhesh1793/code/llm_efficiency_challenge/llm_efficiency_challenge/.venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:428: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7577, 'learning_rate': 0.00016250000000000002, 'epoch': 0.02}\n",
      "{'loss': 0.8182, 'learning_rate': 0.00015833333333333332, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siddhesh1793/code/llm_efficiency_challenge/llm_efficiency_challenge/.venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:428: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8487, 'learning_rate': 0.00015416666666666668, 'epoch': 0.02}\n",
      "{'loss': 1.7609, 'learning_rate': 0.00015000000000000001, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siddhesh1793/code/llm_efficiency_challenge/llm_efficiency_challenge/.venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:428: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2098, 'learning_rate': 0.00014583333333333335, 'epoch': 0.02}\n",
      "{'loss': 2.4736, 'learning_rate': 0.00014166666666666668, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siddhesh1793/code/llm_efficiency_challenge/llm_efficiency_challenge/.venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:428: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5606, 'learning_rate': 0.0001375, 'epoch': 0.03}\n",
      "{'loss': 2.4593, 'learning_rate': 0.00013333333333333334, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siddhesh1793/code/llm_efficiency_challenge/llm_efficiency_challenge/.venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:428: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.324, 'learning_rate': 0.00012916666666666667, 'epoch': 0.03}\n",
      "{'loss': 2.2145, 'learning_rate': 0.000125, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siddhesh1793/code/llm_efficiency_challenge/llm_efficiency_challenge/.venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:428: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9277, 'learning_rate': 0.00012083333333333333, 'epoch': 0.03}\n",
      "{'loss': 1.5122, 'learning_rate': 0.00011666666666666668, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siddhesh1793/code/llm_efficiency_challenge/llm_efficiency_challenge/.venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:428: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3103, 'learning_rate': 0.00011250000000000001, 'epoch': 0.04}\n",
      "{'loss': 2.1635, 'learning_rate': 0.00010833333333333333, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siddhesh1793/code/llm_efficiency_challenge/llm_efficiency_challenge/.venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:428: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9069, 'learning_rate': 0.00010416666666666667, 'epoch': 0.04}\n",
      "{'loss': 2.2187, 'learning_rate': 0.0001, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siddhesh1793/code/llm_efficiency_challenge/llm_efficiency_challenge/.venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:428: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.81, 'learning_rate': 9.583333333333334e-05, 'epoch': 0.04}\n",
      "{'loss': 2.7756, 'learning_rate': 9.166666666666667e-05, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siddhesh1793/code/llm_efficiency_challenge/llm_efficiency_challenge/.venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:428: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6671, 'learning_rate': 8.75e-05, 'epoch': 0.05}\n",
      "{'loss': 2.2795, 'learning_rate': 8.333333333333334e-05, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siddhesh1793/code/llm_efficiency_challenge/llm_efficiency_challenge/.venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:428: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8735, 'learning_rate': 7.916666666666666e-05, 'epoch': 0.05}\n",
      "{'loss': 1.739, 'learning_rate': 7.500000000000001e-05, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siddhesh1793/code/llm_efficiency_challenge/llm_efficiency_challenge/.venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:428: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.559, 'learning_rate': 7.083333333333334e-05, 'epoch': 0.05}\n",
      "{'loss': 1.4029, 'learning_rate': 6.666666666666667e-05, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siddhesh1793/code/llm_efficiency_challenge/llm_efficiency_challenge/.venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:428: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6741, 'learning_rate': 6.25e-05, 'epoch': 0.06}\n",
      "{'loss': 3.0124, 'learning_rate': 5.833333333333334e-05, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siddhesh1793/code/llm_efficiency_challenge/llm_efficiency_challenge/.venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:428: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4609, 'learning_rate': 5.4166666666666664e-05, 'epoch': 0.06}\n",
      "{'loss': 0.9397, 'learning_rate': 5e-05, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siddhesh1793/code/llm_efficiency_challenge/llm_efficiency_challenge/.venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:428: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8687, 'learning_rate': 4.5833333333333334e-05, 'epoch': 0.06}\n",
      "{'loss': 1.4135, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siddhesh1793/code/llm_efficiency_challenge/llm_efficiency_challenge/.venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:428: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3522, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.07}\n",
      "{'loss': 2.4627, 'learning_rate': 3.3333333333333335e-05, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siddhesh1793/code/llm_efficiency_challenge/llm_efficiency_challenge/.venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:428: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7225, 'learning_rate': 2.916666666666667e-05, 'epoch': 0.07}\n",
      "{'loss': 1.2857, 'learning_rate': 2.5e-05, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siddhesh1793/code/llm_efficiency_challenge/llm_efficiency_challenge/.venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:428: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2648, 'learning_rate': 2.0833333333333336e-05, 'epoch': 0.07}\n",
      "{'loss': 1.9939, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siddhesh1793/code/llm_efficiency_challenge/llm_efficiency_challenge/.venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:428: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5065, 'learning_rate': 1.25e-05, 'epoch': 0.07}\n",
      "{'loss': 2.0672, 'learning_rate': 8.333333333333334e-06, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siddhesh1793/code/llm_efficiency_challenge/llm_efficiency_challenge/.venv/lib/python3.8/site-packages/torch/utils/checkpoint.py:428: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0336, 'learning_rate': 4.166666666666667e-06, 'epoch': 0.08}\n",
      "{'loss': 1.771, 'learning_rate': 0.0, 'epoch': 0.08}\n",
      "{'train_runtime': 269.0258, 'train_samples_per_second': 0.743, 'train_steps_per_second': 0.186, 'train_loss': 2.1216299855709075, 'epoch': 0.08}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=50, training_loss=2.1216299855709075, metrics={'train_runtime': 269.0258, 'train_samples_per_second': 0.743, 'train_steps_per_second': 0.186, 'train_loss': 2.1216299855709075, 'epoch': 0.08})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "# needed for gpt-neo-x tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=data[\"train\"],\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=2,\n",
    "        max_steps=50,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=1,\n",
    "        output_dir=f\"outputs/{model_name}\",\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        save_steps=2,\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
